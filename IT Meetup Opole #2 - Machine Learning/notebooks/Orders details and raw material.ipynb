{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danone - IT Meetup Opole # 2\n",
    "\n",
    "### This Jupyter notebook explains the steps followed to obtain the orders details and the raw material used on the production of different recipes.\n",
    "### The original source of the data is an Excel file, manually prepared using paper documents and existing systems in the factories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Danone](../img/Danone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prearing working environment\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter, column_index_from_string\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r'/home/hack/utils')\n",
    "# Loading internal file with DB connections details\n",
    "from db_connections import my_engine\n",
    "\n",
    "## I'm using sqlalchemy for MS SQL Server + pyodbc\n",
    "## engine = create_engine('mssql+pyodbc://user:password@server/database?driver=ODBC+Driver+17+for+SQL+Server', fast_executemany=True)\n",
    "## Since SQLAlchemy 1.3.0, released 2019-03-04, sqlalchemy now supports engine = create_engine(sqlalchemy_url, fast_executemany=True) for the mssql+pyodbc dialect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The original Excel file was not prepared in a standard table form. Some minor adjustments, using Excel functionalities, were performed to achieve a conventional dataframe-like table structure.\n",
    "\n",
    "![order_details_modifications](img/orders_details.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the excel file:\n",
    "orders = pd.read_excel(r'../excel_files/orders_data.xlsx', sheet_name='GENERAL', skiprows = 1, usecols = 'A:Q')\n",
    "\n",
    "# renaming original columns names in Polish to a standard English version - easier to work with\n",
    "orders.rename(columns={'ID':'id',\n",
    "                       'Linia':'line_dd',\n",
    "                       'Numer zlecenia':'process_order',\n",
    "                       'Partia SAP':'process_order_sap',\n",
    "                       'Partia MES':'process_order_mes',\n",
    "                       'Materiał':'material_code',\n",
    "                       'Receptura':'recipe',\n",
    "                       'Data aktywacji':'activation_date',\n",
    "                       'Data zamknięcia':'closing_date',\n",
    "                       'Materiał2':'material_sub',\n",
    "                       'Partia SAP3':'process_order_sap3',\n",
    "                       'Usage percentage':'usage_pct',\n",
    "                       'Tłuszcz [%]':'fat_pct',\n",
    "                       'Granulacja [um] 200um though':'particles_grp1',\n",
    "                       'Granulacja [um]\\n200um retained':'particles_grp2',\n",
    "                       'Granulacja [um]\\n500um retained':'particles_grp3',\n",
    "                       'Wilgotność [g/100g]':'humidity'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic information defining the process order and production scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_details = orders.loc[:,['id','line_dd','process_order','process_order_sap','material_code','recipe','activation_date','closing_date']]\n",
    "orders_details = orders_details.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information about the raw material(s) used on the recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_material_used = orders.loc[:,['id','material_sub','process_order_sap3','usage_pct']]\n",
    "raw_material_used = raw_material_used.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics of the INput materials into the recipe processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_material_in = orders.loc[:,['id','process_order_sap3','fat_pct','particles_grp1','particles_grp2','particles_grp3','humidity']]\n",
    "raw_material_in = raw_material_in.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the sake of this excercise, and since the data used here is small, we are saving the data as \"df.csv\" it is accessible in this repository in the *processed_data* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_details.to_csv(\"../processed_data/orders_details.csv\")\n",
    "raw_material_used.to_csv(\"../processed_data/raw_material_used.csv\")\n",
    "raw_material_in.to_csv(\"../processed_data/raw_material_in.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the Hackathon the data will be available through a MS SQL Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders_details - witing to DB using pandas\n",
    "orders_details.to_sql('orders_details', my_engine, if_exists='replace', index=False)\n",
    "\n",
    "# raw_material_used - witing to DB using pandas\n",
    "raw_material_used.to_sql('raw_material_used', my_engine, if_exists='replace', index=False)\n",
    "\n",
    "# raw_material_in - witing to DB using pandas\n",
    "raw_material_in.to_sql('raw_material_in', my_engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have the data already in the DB, and thanks to panda we didnt have to create the tables by ourselves. However, to have the proper data model we need to define the necessary primary and foreing keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fd531ebf048>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection = my_engine.connect()\n",
    "# First we need to declare the column used as primary key as NOT NULL\n",
    "connection.execute(\"ALTER TABLE orders_details ALTER COLUMN id INT NOT NULL\")\n",
    "# Now it is possible to alter the table and set the _id_ as primary key\n",
    "connection.execute(\"ALTER TABLE orders_details ADD CONSTRAINT orders_details_pk PRIMARY KEY (id)\")\n",
    "\n",
    "## For raw_material_used we set-up orders_details.id as Foreing key\n",
    "# declaring as not nulleable the columns to be used as primary key\n",
    "connection.execute(\"ALTER TABLE raw_material_used ALTER COLUMN id INT NOT NULL\")\n",
    "connection.execute(\"ALTER TABLE raw_material_used ALTER COLUMN material_sub INT NOT NULL\")\n",
    "connection.execute(\"ALTER TABLE raw_material_used ALTER COLUMN process_order_sap3 INT NOT NULL\")\n",
    "# to avoid creating artificial columns, a group of exisitng columns is used as primary key\n",
    "# we could have kept index=True when sending the data to DB, and used as id. \n",
    "connection.execute(\"ALTER TABLE raw_material_used ADD CONSTRAINT raw_material_used_pk PRIMARY KEY (id,material_sub,process_order_sap3)\")\n",
    "# finally, the foreing key is created\n",
    "connection.execute(\"ALTER TABLE raw_material_used ADD CONSTRAINT fk_raw_material_used_id FOREIGN KEY (id) REFERENCES orders_details (id) ON DELETE NO ACTION\")\n",
    "\n",
    "## For raw_material_in we set-up orders_details.id as Foreing key\n",
    "# declaring as not nulleable the columns to be used as primary key\n",
    "connection.execute(\"ALTER TABLE raw_material_in ALTER COLUMN id INT NOT NULL\")\n",
    "connection.execute(\"ALTER TABLE raw_material_in ALTER COLUMN process_order_sap3 INT NOT NULL\")\n",
    "# as before, we use a combination of columns as our primary\n",
    "connection.execute(\"ALTER TABLE raw_material_in ADD CONSTRAINT raw_material_in_pk PRIMARY KEY (id,process_order_sap3)\")\n",
    "connection.execute(\"ALTER TABLE raw_material_used ADD CONSTRAINT fk_raw_material_in_id FOREIGN KEY (id) REFERENCES orders_details (id) ON DELETE NO ACTION\")\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
