{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danone - IT Meetup Opole # 2\n",
    "\n",
    "### This Jupyter notebook explains the steps followed to obtain the recipe processing  and the meassurements from the output of the production of different recipes.\n",
    "### The original source of the data is an Excel file, manually prepared using paper documents and existing systems in the factories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./Danone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prearing working environment\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter, column_index_from_string\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r'/home/hack/utils')\n",
    "# Loading internal file with DB connections details\n",
    "from db_connections import my_engine\n",
    "\n",
    "## I'm using sqlalchemy for MS SQL Server + pyodbc\n",
    "## engine = create_engine('mssql+pyodbc://user:password@server/database?driver=ODBC+Driver+17+for+SQL+Server', fast_executemany=True)\n",
    "## Since SQLAlchemy 1.3.0, released 2019-03-04, sqlalchemy now supports engine = create_engine(sqlalchemy_url, fast_executemany=True) for the mssql+pyodbc dialect.\n",
    "\n",
    "# We also need to load orders_details from our DB. It contains the Production orders under consideration\n",
    "orders_details = pd.read_sql(\"SELECT * FROM orders_details\", my_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the data is presented in Excel, sheet 'Arkusz1'. There are multiple tables, and it is required to scan all the cells and find the tables in it.\n",
    "\n",
    "### The strategy implemented here, after a visual inspection, uses the Production Order as a reference cell, and then each of the tables in the right side is read, and it values extracted in a panda dataframe. Using openpyxl, this task gets very logical.\n",
    "\n",
    "![processing_and_out_data](img/processing_details.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the workbook\n",
    "wb_data = load_workbook(r'../excel_files/in_process_out_data.xlsx')\n",
    "# Selecting the sheet of interest\n",
    "sheet_data = wb_data['Arkusz1']\n",
    "\n",
    "# Creating a dataframe to keep the results from scanning the Excel file to locate and extract the production order \n",
    "temp_df = pd.DataFrame({'coordinate':[],'process_order_2':[],'row':[],'column':[],'column_letter':[],'process_order_2_header':[], 'process_order':[]})\n",
    "\n",
    "# Finding the cells that contain 'Production Order'\n",
    "column_number = column_index_from_string('B') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over all the column B and to the last used cell\n",
    "for ii in range(1, sheet_data.max_row):\n",
    "    c = sheet_data.cell(row=ii, column=column_number)\n",
    "    if c.value is None:\n",
    "        ... # do nothing if cell is empty - important condition to avoid c.value.split breaking when cell is empty.\n",
    "        \n",
    "    elif c.value.startswith('Production Order:'):\n",
    "        production_order = c.value.split(':')[1].strip()\n",
    "        c2 = sheet_data.cell(row=ii+2, column=column_index_from_string('C'))\n",
    "        c3 = sheet_data.cell(row=ii+4, column=column_index_from_string('C'))\n",
    "        temp_df = temp_df.append({'coordinate':c.coordinate,\n",
    "                                  'process_order_2':int(production_order),\n",
    "                                  'row': c.row,\n",
    "                                  'column':c.column,\n",
    "                                  'column_letter':c.column_letter,\n",
    "                                  'process_order_2_header': c2.value,\n",
    "                                  'process_order': int(c3.value)},ignore_index=True)\n",
    "\n",
    "temp_df['test'] = temp_df ['process_order'] == temp_df['process_order_2']\n",
    "\n",
    "# Merging the Production orders found with the expected from orders detail\n",
    "orders_consolidated = pd.merge(orders_details, temp_df, how = 'left', on = 'process_order')\n",
    "\n",
    "orders_consolidated = orders_consolidated.dropna()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the tables we want to replicate from the Excel file into Pandas dataframe\n",
    "## recipe processing data\n",
    "processing_details_slurry = pd.DataFrame({'orders_details_id':[],'slurry_process_order':[], 'slurry_line':[], 'slurry_start_time':[], 'water_pct':[], 'water_correction':[]})\n",
    "processing_details_bigbag = pd.DataFrame({'orders_details_id':[],'bigbag_number':[], 'bigbag_filling_time_end':[], 'sifter_speed_nominal_pct':[]})\n",
    "processing_details_dd = pd.DataFrame({'orders_details_id':[],'testing_time':[], 'steam_preasure':[], 'dd_speed':[], 'temp_out':[]})\n",
    "## out and semi-finished product data\n",
    "out_test_during_production = pd.DataFrame({'orders_details_id':[],'line':[], 'process_order':[], 'testing_time':[], 'humidity':[], 'bulk_density':[]})\n",
    "out_semi_finished_procution = pd.DataFrame({'orders_details_id':[],'bigbag_number':[], 'bigbag_filling_time_end':[], 'bigbag_filling_duration':[], 'bigbag_weight':[], 'efficiency':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the visual inspection we did to the Excel file, we locate the columns where each of the tables begins\n",
    "slurry_column = column_index_from_string('N')\n",
    "bigbag_column = column_index_from_string('S')\n",
    "dd_column = column_index_from_string('V')\n",
    "out_test_column = column_index_from_string('AA')\n",
    "out_semi_column = column_index_from_string('AH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we iterate over each row in our consolidated production orders\n",
    "\n",
    "for i,row in orders_consolidated.iterrows():\n",
    "    # the headers of our tables are 2 rows below from the cell that contains the production order\n",
    "    process_order_row = int(row.loc['row'])+2    \n",
    "    \n",
    "    # we load the first header for each of our tables within the Excel file\n",
    "    slurry_cell = sheet_data.cell(row= process_order_row, column= slurry_column)\n",
    "    bigbag_cell = sheet_data.cell(row= process_order_row, column= bigbag_column)\n",
    "    dd_cell = sheet_data.cell(row= process_order_row, column= dd_column)\n",
    "    out_test_cell = sheet_data.cell(row= process_order_row, column= out_test_column)\n",
    "    out_semi_cell = sheet_data.cell(row= process_order_row, column= out_semi_column)\n",
    "    \n",
    "    if 'Slurry batch number' in slurry_cell.value: # double check that we are actually in the cell we expect\n",
    "        slurry = True\n",
    "        # the first value of the table is in a cell 2 rows below the header \n",
    "        ii = process_order_row + 2\n",
    "        while slurry == True:\n",
    "            # we can safely scan the values until an empty cell is found\n",
    "            # each column having as a reference the first header - in this case: 'Slurry batch number'\n",
    "            slurry_process_order = sheet_data.cell(row= ii, column= slurry_column).value\n",
    "            slurry_line = sheet_data.cell(row= ii, column= slurry_column + 1).value\n",
    "            slurry_start_time = sheet_data.cell(row= ii, column= slurry_column + 2).value\n",
    "            water_pct = sheet_data.cell(row= ii, column= slurry_column + 3).value\n",
    "            water_correction = sheet_data.cell(row= ii, column= slurry_column + 4).value\n",
    "               \n",
    "            if slurry_process_order is None:\n",
    "                slurry = False\n",
    "            else:\n",
    "                processing_details_slurry = processing_details_slurry.append({\n",
    "                        'orders_details_id':int(row.loc['id']),\n",
    "                        'slurry_process_order':slurry_process_order,\n",
    "                        'slurry_line':slurry_line,\n",
    "                        'slurry_start_time':slurry_start_time,\n",
    "                        'water_pct':water_pct,\n",
    "                        'water_correction':water_correction},ignore_index=True)\n",
    "                ii += 1\n",
    "    \n",
    "    if 'Big Bag number' in bigbag_cell.value:\n",
    "        bigbag = True\n",
    "        ii = process_order_row + 2\n",
    "        \n",
    "        while bigbag == True:\n",
    "            bigbag_number = sheet_data.cell(row= ii, column= bigbag_column).value\n",
    "            bigbag_filling_time_end = sheet_data.cell(row= ii, column= bigbag_column + 1).value\n",
    "            sifter_speed_nominal_pct = sheet_data.cell(row= ii, column= bigbag_column + 2).value\n",
    "\n",
    "            if bigbag_number is None:\n",
    "                bigbag = False\n",
    "            else:\n",
    "                processing_details_bigbag = processing_details_bigbag.append({\n",
    "                        'orders_details_id':int(row.loc['id']),\n",
    "                        'bigbag_number': bigbag_number,\n",
    "                        'bigbag_filling_time_end': bigbag_filling_time_end,\n",
    "                        'sifter_speed_nominal_pct':sifter_speed_nominal_pct},ignore_index=True)\n",
    "                ii += 1                   \n",
    "            \n",
    "        \n",
    "    if 'Testing time' in dd_cell.value:\n",
    "        dd = True\n",
    "        ii = process_order_row + 2\n",
    "                \n",
    "        while dd == True:\n",
    "            testing_time = sheet_data.cell(row= ii, column= dd_column).value\n",
    "            steam_preasure = sheet_data.cell(row= ii, column= dd_column + 1).value\n",
    "            dd_speed = sheet_data.cell(row= ii, column= dd_column + 2).value\n",
    "            temp_out = sheet_data.cell(row= ii, column= dd_column + 3).value\n",
    "\n",
    "            if testing_time is None:\n",
    "                dd = False\n",
    "            else:\n",
    "                processing_details_dd = processing_details_dd.append({\n",
    "                        'orders_details_id':int(row.loc['id']),\n",
    "                        'testing_time':testing_time,\n",
    "                        'steam_preasure':steam_preasure,\n",
    "                        'dd_speed':dd_speed,\n",
    "                        'temp_out':temp_out},ignore_index=True)\n",
    "                ii += 1  \n",
    "        \n",
    "    if 'Line' in out_test_cell.value:\n",
    "        out_test = True\n",
    "        ii = process_order_row + 2\n",
    "                \n",
    "        while out_test == True:\n",
    "            line = sheet_data.cell(row= ii, column= out_test_column).value\n",
    "            process_order = sheet_data.cell(row= ii, column= out_test_column + 1).value\n",
    "            testing_time = sheet_data.cell(row= ii, column= out_test_column + 2).value\n",
    "            ## Skip out_test_column + 3\n",
    "            humidity = sheet_data.cell(row= ii, column= out_test_column + 4).value\n",
    "            bulk_density = sheet_data.cell(row= ii, column= out_test_column + 5).value\n",
    "\n",
    "            if line is None:\n",
    "                out_test = False\n",
    "            else:\n",
    "                out_test_during_production = out_test_during_production.append({\n",
    "                        'orders_details_id':int(row.loc['id']),\n",
    "                        'line':line,\n",
    "                        'process_order':process_order,\n",
    "                        'testing_time':testing_time,\n",
    "                        'humidity':humidity,\n",
    "                        'bulk_density':bulk_density},ignore_index=True)\n",
    "    \n",
    "                ii += 1\n",
    "    \n",
    "    if 'Big Bag number' in out_semi_cell.value:\n",
    "        out_semi = True\n",
    "        ii = process_order_row + 2\n",
    "          \n",
    "        while out_semi == True:\n",
    "            bigbag_number = sheet_data.cell(row= ii, column= out_semi_column).value\n",
    "            bigbag_filling_time_end = sheet_data.cell(row= ii, column= out_semi_column + 1).value\n",
    "            bigbag_filling_duration = sheet_data.cell(row= ii, column= out_semi_column + 2).value\n",
    "            bigbag_weight = sheet_data.cell(row= ii, column= out_semi_column + 3).value\n",
    "            efficiency = sheet_data.cell(row= ii, column= out_semi_column + 4).value\n",
    "            \n",
    "            if bigbag_number is None:\n",
    "                out_semi = False\n",
    "            else:\n",
    "                out_semi_finished_procution = out_semi_finished_procution.append({\n",
    "                        'orders_details_id':int(row.loc['id']),\n",
    "                        'bigbag_number': bigbag_number,\n",
    "                        'bigbag_filling_time_end':bigbag_filling_time_end,\n",
    "                        'bigbag_filling_duration':bigbag_filling_duration,\n",
    "                        'bigbag_weight':bigbag_weight,\n",
    "                        'efficiency':efficiency}, ignore_index=True)\n",
    "    \n",
    "                ii += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the scanned tables as csv and sending each to the DB\n",
    "processing_details_slurry.to_csv(r'../processed_data/processing_details_slurry.csv')\n",
    "processing_details_slurry.to_sql('processing_details_slurry', my_engine, if_exists='replace', index=False)\n",
    "\n",
    "processing_details_bigbag.to_csv(r'../processed_data/processing_details_bigbag.csv')\n",
    "processing_details_bigbag.to_sql('processing_details_bigbag', my_engine, if_exists='replace', index=False)\n",
    "\n",
    "processing_details_dd = processing_details_dd.drop_duplicates()\n",
    "processing_details_dd.to_csv(r'../processed_data/processing_details_dd.csv')\n",
    "processing_details_dd.to_sql('processing_details_dd', my_engine, if_exists='replace', index=False)\n",
    "\n",
    "out_test_during_production.to_csv(r'../processed_data/out_test_during_production.csv')\n",
    "out_test_during_production.to_sql('out_test_during_production', my_engine, if_exists='replace', index=False)\n",
    "\n",
    "out_semi_finished_procution.to_csv(r'../processed_data/out_semi_finished_procution.csv') \n",
    "out_semi_finished_procution.to_sql('out_semi_finished_procution', my_engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have the proper data model we need to define the necessary primary and foreing keys\n",
    "connection = my_engine.connect()\n",
    "## processing_details_slurry\n",
    "# First we need to declare the columns used as primary key as NOT NULL\n",
    "connection.execute(\"ALTER TABLE processing_details_slurry ALTER COLUMN orders_details_id INT NOT NULL\")\n",
    "connection.execute(\"ALTER TABLE processing_details_slurry ALTER COLUMN slurry_process_order INT NOT NULL\")\n",
    "# pk\n",
    "connection.execute(\"ALTER TABLE processing_details_slurry ADD CONSTRAINT processing_details_slurry_pk PRIMARY KEY (orders_details_id, slurry_process_order)\")\n",
    "# fk\n",
    "connection.execute(\"ALTER TABLE processing_details_slurry ADD CONSTRAINT fk_processing_details_slurry FOREIGN KEY (orders_details_id) REFERENCES orders_details (id) ON DELETE NO ACTION\")\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = my_engine.connect()\n",
    "## processing_details_bigbag\n",
    "# First we need to declare the columns used as primary key as NOT NULL\n",
    "connection.execute(\"ALTER TABLE processing_details_bigbag ALTER COLUMN orders_details_id INT NOT NULL\")\n",
    "connection.execute(\"ALTER TABLE processing_details_bigbag ALTER COLUMN bigbag_number INT NOT NULL\")\n",
    "# pk\n",
    "connection.execute(\"ALTER TABLE processing_details_bigbag ADD CONSTRAINT processing_details_bigbag_pk PRIMARY KEY (orders_details_id, bigbag_number)\")\n",
    "# fk\n",
    "connection.execute(\"ALTER TABLE processing_details_bigbag ADD CONSTRAINT fk_processing_details_bigbag FOREIGN KEY (orders_details_id) REFERENCES orders_details (id) ON DELETE NO ACTION\")\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = my_engine.connect()\n",
    "## processing_details_dd\n",
    "# First we need to declare the columns used as primary key as NOT NULL\n",
    "connection.execute(\"ALTER TABLE processing_details_dd ALTER COLUMN orders_details_id INT NOT NULL\")\n",
    "connection.execute(\"ALTER TABLE processing_details_dd ALTER COLUMN testing_time datetime NOT NULL\")\n",
    "# pk\n",
    "connection.execute(\"ALTER TABLE processing_details_dd ADD CONSTRAINT processing_details_dd_pk PRIMARY KEY (orders_details_id, testing_time)\")\n",
    "# fk\n",
    "connection.execute(\"ALTER TABLE processing_details_dd ADD CONSTRAINT fk_processing_details_dd FOREIGN KEY (orders_details_id) REFERENCES orders_details (id) ON DELETE NO ACTION\")\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = my_engine.connect()\n",
    "## out_test_during_production\n",
    "# First we need to declare the columns used as primary key as NOT NULL\n",
    "connection.execute(\"ALTER TABLE out_test_during_production ALTER COLUMN orders_details_id INT NOT NULL\")\n",
    "connection.execute(\"ALTER TABLE out_test_during_production ALTER COLUMN testing_time datetime NOT NULL\")\n",
    "# pk\n",
    "connection.execute(\"ALTER TABLE out_test_during_production ADD CONSTRAINT out_test_during_production_pk PRIMARY KEY (orders_details_id, testing_time)\")\n",
    "# fk\n",
    "connection.execute(\"ALTER TABLE out_test_during_production ADD CONSTRAINT fk_out_test_during_production FOREIGN KEY (orders_details_id) REFERENCES orders_details (id) ON DELETE NO ACTION\")\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = my_engine.connect()\n",
    "## out_semi_finished_procution\n",
    "# First we need to declare the columns used as primary key as NOT NULL\n",
    "connection.execute(\"ALTER TABLE out_semi_finished_procution ALTER COLUMN orders_details_id INT NOT NULL\")\n",
    "connection.execute(\"ALTER TABLE out_semi_finished_procution ALTER COLUMN bigbag_number datetime NOT NULL\")\n",
    "# pk\n",
    "connection.execute(\"ALTER TABLE out_semi_finished_procution ADD CONSTRAINT out_semi_finished_procution_pk PRIMARY KEY (orders_details_id, bigbag_number)\")\n",
    "# fk\n",
    "connection.execute(\"ALTER TABLE out_semi_finished_procution ADD CONSTRAINT fk_out_semi_finished_procution FOREIGN KEY (orders_details_id) REFERENCES orders_details (id) ON DELETE NO ACTION\")\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
